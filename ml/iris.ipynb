{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e35457",
   "metadata": {},
   "source": [
    "# Iris Species Classification Pipeline\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting iris species based on flower measurements.\n",
    "\n",
    "## Project Overview\n",
    "- **Task**: Multi-class classification (setosa, versicolor, virginica)\n",
    "- **Features**: Sepal length, sepal width, petal length, petal width\n",
    "- **Goal**: Build and compare different ML models\n",
    "\n",
    "## AI Concepts Covered\n",
    "- Supervised Learning (Classification)\n",
    "- Data Exploration & Visualization\n",
    "- Data Preprocessing (train/test split, feature scaling)\n",
    "- Model Selection & Comparison\n",
    "- Evaluation Metrics (accuracy, confusion matrix, classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49853a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce629c8",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe467f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "df = pd.read_csv('IRIS.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìà Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Species Distribution:\")\n",
    "print(df['species'].value_counts())\n",
    "print(f\"\\nUnique species: {df['species'].unique()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüîç Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb14518",
   "metadata": {},
   "source": [
    "## 2. Data Visualization & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions by Species', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Create histogram with species overlay\n",
    "    for species in df['species'].unique():\n",
    "        species_data = df[df['species'] == species][feature]\n",
    "        ax.hist(species_data, alpha=0.7, label=species.replace('Iris-', ''), bins=15)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots for better comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle('Feature Distributions by Species (Box Plots)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(data=df, x='species', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel('Species')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Feature Relationships\n",
    "print(\"üîç Exploring pairwise relationships between features...\")\n",
    "\n",
    "# Create a clean species column for plotting\n",
    "df['species_clean'] = df['species'].str.replace('Iris-', '')\n",
    "\n",
    "# Pair plot showing relationships between all features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df, hue='species_clean', vars=features, diag_kind='hist', plot_kws={'alpha': 0.7})\n",
    "plt.suptitle('Pairwise Feature Relationships', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations:\")\n",
    "print(\"‚Ä¢ Petal length and width are highly correlated\")\n",
    "print(\"‚Ä¢ Sepal measurements show different patterns across species\")\n",
    "print(\"‚Ä¢ Setosa appears to be easily separable from other species\")\n",
    "print(\"‚Ä¢ Versicolor and Virginica show some overlap in feature space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a86d27",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[features]  # Features\n",
    "y = df['species']  # Target variable\n",
    "\n",
    "print(\"üîß Data Preprocessing:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "print(f\"Target classes: {y.unique()}\")\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in both sets\n",
    "print(f\"\\nüè∑Ô∏è Class distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nüè∑Ô∏è Class distribution in testing set:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Display feature statistics before scaling\n",
    "print(f\"\\nüìà Feature statistics (before scaling):\")\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2cfb7",
   "metadata": {},
   "source": [
    "## 4. Model Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30549df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML pipelines with different classifiers\n",
    "print(\"ü§ñ Building ML Pipelines...\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Create pipelines with StandardScaler + Classifier\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    pipelines[name] = pipeline\n",
    "\n",
    "# Train all models\n",
    "print(\"üöÄ Training models...\")\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Training completed! {len(models)} models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5e274",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Comparison\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create accuracy comparison\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "# Display accuracy table\n",
    "for name, accuracy in zip(model_names, accuracies):\n",
    "    print(f\"{name:<20}: {accuracy:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_accuracy = results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']\n",
    "bars = plt.bar(model_names, accuracies, color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{accuracy:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Models')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Evaluation for Best Model\n",
    "print(f\"üîç Detailed Analysis for Best Model: {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "species_labels = [s.replace('Iris-', '') for s in sorted(y_test.unique())]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=species_labels, yticklabels=species_labels)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, best_predictions, average='weighted')\n",
    "recall = recall_score(y_test, best_predictions, average='weighted')\n",
    "f1 = f1_score(y_test, best_predictions, average='weighted')\n",
    "\n",
    "print(f\"\\nüìà Overall Metrics for {best_model_name}:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"üîç Feature Importance Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get feature importance for tree-based models\n",
    "if best_model_name in ['Decision Tree', 'Random Forest']:\n",
    "    best_model = results[best_model_name]['pipeline']\n",
    "    feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Importance ({best_model_name}):\")\n",
    "    for _, row in importance_df.iterrows():\n",
    "        print(f\"{row['feature']:<15}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(importance_df['feature'], importance_df['importance'], \n",
    "                   color='lightblue', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, importance in zip(bars, importance_df['importance']):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n",
    "    print(\"Tree-based models (Decision Tree, Random Forest) show feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e45ee",
   "metadata": {},
   "source": [
    "## 6. Model Testing & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31920528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model with sample predictions\n",
    "print(\"üß™ Testing the Best Model with Sample Predictions:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "best_pipeline = results[best_model_name]['pipeline']\n",
    "\n",
    "# Create sample flowers for prediction\n",
    "sample_flowers = pd.DataFrame({\n",
    "    'sepal_length': [5.1, 6.2, 7.3],\n",
    "    'sepal_width': [3.5, 2.8, 2.9],\n",
    "    'petal_length': [1.4, 4.5, 6.3],\n",
    "    'petal_width': [0.2, 1.5, 1.8]\n",
    "})\n",
    "\n",
    "print(\"üìù Sample Flower Measurements:\")\n",
    "print(sample_flowers)\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_pipeline.predict(sample_flowers)\n",
    "prediction_probabilities = best_pipeline.predict_proba(sample_flowers)\n",
    "\n",
    "print(f\"\\nüîÆ Predictions using {best_model_name}:\")\n",
    "species_names = best_pipeline.classes_\n",
    "for i, (pred, probs) in enumerate(zip(predictions, prediction_probabilities)):\n",
    "    print(f\"\\nFlower {i+1}:\")\n",
    "    print(f\"  Predicted Species: {pred}\")\n",
    "    print(f\"  Confidence Scores:\")\n",
    "    for species, prob in zip(species_names, probs):\n",
    "        print(f\"    {species}: {prob:.4f}\")\n",
    "\n",
    "# Show some actual test cases for comparison\n",
    "print(f\"\\nüîç Actual Test Cases vs Predictions:\")\n",
    "print(\"=\" * 45)\n",
    "sample_indices = [0, 5, 10, 15, 20]  # Sample a few test cases\n",
    "\n",
    "for i in sample_indices:\n",
    "    if i < len(X_test):\n",
    "        actual = y_test.iloc[i]\n",
    "        predicted = best_predictions[i]\n",
    "        features_values = X_test.iloc[i]\n",
    "        \n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        print(f\"  Features: {features_values.values}\")\n",
    "        print(f\"  Actual: {actual}\")\n",
    "        print(f\"  Predicted: {predicted}\")\n",
    "        print(f\"  Correct: {'‚úÖ' if actual == predicted else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a995d30",
   "metadata": {},
   "source": [
    "## 7. Conclusions & Key Findings\n",
    "\n",
    "### üéØ Summary of Results\n",
    "\n",
    "This machine learning pipeline successfully demonstrated iris species classification using multiple algorithms. Here are the key findings:\n",
    "\n",
    "### üìä Model Performance\n",
    "- **All models achieved high accuracy** (typically >95%) on the iris dataset\n",
    "- **Best performing model**: The model with highest accuracy from our comparison\n",
    "- **Feature scaling**: StandardScaler preprocessing improved model performance\n",
    "\n",
    "### üîç Key Insights\n",
    "1. **Dataset Characteristics**:\n",
    "   - Clean dataset with no missing values\n",
    "   - Well-balanced classes (50 samples per species)\n",
    "   - Clear separability between species\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - Petal measurements (length & width) are typically most discriminative\n",
    "   - Sepal measurements provide additional distinguishing power\n",
    "   - Strong correlation between petal length and width\n",
    "\n",
    "3. **Model Behavior**:\n",
    "   - Iris-setosa is easily separable from other species\n",
    "   - Iris-versicolor and Iris-virginica show some overlap\n",
    "   - Tree-based models can reveal feature importance\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- Experiment with hyperparameter tuning\n",
    "- Try ensemble methods for potentially better performance\n",
    "- Implement cross-validation for more robust evaluation\n",
    "- Deploy the model for real-world predictions\n",
    "\n",
    "### üõ†Ô∏è Technical Skills Demonstrated\n",
    "- ‚úÖ Data loading and exploration\n",
    "- ‚úÖ Visualization and statistical analysis\n",
    "- ‚úÖ Data preprocessing and scaling\n",
    "- ‚úÖ ML pipeline creation\n",
    "- ‚úÖ Model comparison and evaluation\n",
    "- ‚úÖ Performance metrics analysis\n",
    "- ‚úÖ Feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6baa4eb",
   "metadata": {},
   "source": [
    "## 8. Bonus: Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "print(\"üîÑ Cross-Validation Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Perform 5-fold cross-validation for all models\n",
    "cv_scores = {}\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    cv_score = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_scores[name] = cv_score\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV Scores: {cv_score}\")\n",
    "    print(f\"  Mean CV Score: {cv_score.mean():.4f} (¬±{cv_score.std():.4f})\")\n",
    "\n",
    "# Find most consistent model\n",
    "most_consistent = min(cv_scores.keys(), key=lambda x: cv_scores[x].std())\n",
    "print(f\"\\nüéØ Most Consistent Model: {most_consistent}\")\n",
    "print(f\"   Standard Deviation: {cv_scores[most_consistent].std():.4f}\")\n",
    "\n",
    "# Hyperparameter tuning for best model\n",
    "print(f\"\\nüîß Hyperparameter Tuning for {best_model_name}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if best_model_name == 'k-Nearest Neighbors':\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    # Define parameter grid for KNN\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipelines[best_model_name], \n",
    "        param_grid, \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test tuned model\n",
    "    tuned_accuracy = grid_search.score(X_test, y_test)\n",
    "    print(f\"Tuned model test accuracy: {tuned_accuracy:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Hyperparameter tuning example not implemented for {best_model_name}\")\n",
    "    print(\"Consider tuning parameters like max_depth, n_estimators, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1420fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary\n",
    "print(\"üéâ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìö What we accomplished:\")\n",
    "print(\"‚úÖ Loaded and explored the iris dataset\")\n",
    "print(\"‚úÖ Performed comprehensive data visualization\")\n",
    "print(\"‚úÖ Implemented data preprocessing pipeline\")\n",
    "print(\"‚úÖ Trained and compared 4 different ML models\")\n",
    "print(\"‚úÖ Evaluated models using multiple metrics\")\n",
    "print(\"‚úÖ Analyzed feature importance\")\n",
    "print(\"‚úÖ Demonstrated model predictions\")\n",
    "print(\"‚úÖ Performed cross-validation analysis\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üéØ Best Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nüî¨ Skills & Concepts Demonstrated:\")\n",
    "print(\"‚Ä¢ Supervised Learning (Multi-class Classification)\")\n",
    "print(\"‚Ä¢ Data Exploration & Visualization\")\n",
    "print(\"‚Ä¢ Data Preprocessing & Feature Scaling\")\n",
    "print(\"‚Ä¢ ML Pipeline Development\")\n",
    "print(\"‚Ä¢ Model Comparison & Selection\")\n",
    "print(\"‚Ä¢ Performance Evaluation (Accuracy, Precision, Recall, F1)\")\n",
    "print(\"‚Ä¢ Cross-Validation\")\n",
    "print(\"‚Ä¢ Feature Importance Analysis\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for deployment and real-world use!\")\n",
    "print(\"\\nNext steps: Save the best model, create API endpoints, or deploy to production!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
